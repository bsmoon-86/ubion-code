{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38eed660",
   "metadata": {},
   "source": [
    "# ⚡ XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "XGBoost는 **Gradient Boosting** 알고리즘을 고도화한 라이브러리로,  \n",
    "빠른 속도, 높은 성능, 과적합 제어 기능을 제공하는 **트리 기반 앙상블 학습기**입니다.  \n",
    "대규모 데이터와 Kaggle 대회에서 자주 사용되는 강력한 모델입니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) 주요 클래스\n",
    "- **xgboost.XGBClassifier** : 분류용\n",
    "- **xgboost.XGBRegressor** : 회귀용\n",
    "\n",
    "---\n",
    "\n",
    "## 2) 주요 매개변수 (Parameters, 기본값 포함)\n",
    "\n",
    "## 📌 1. 학습 제어 관련\n",
    "- **n_estimators (기본=100)**  \n",
    "  - 부스팅 단계(트리) 개수. 많을수록 성능↑, 계산량↑, 과적합 위험↑.  \n",
    "  - `learning_rate`와 trade-off 관계.\n",
    "\n",
    "- **learning_rate (eta, 기본=0.1)**  \n",
    "  - 단계별 학습률(수축 계수).  \n",
    "  - 값↓ → 일반화↑, 더 많은 트리 필요. (예: 0.01~0.3 권장)  \n",
    "  - 보통 `n_estimators`와 함께 Grid/Random Search로 조정.\n",
    "- **early_stopping_rounds** : int, optional  \n",
    "  - `eval_set`에서 지정된 평가 지표가 **n 라운드 연속 개선되지 않으면 학습 중단**  \n",
    "  - `n_estimators`는 크게 잡고, 이 값으로 최적 라운드 탐색  \n",
    "- **objective**  \n",
    "  - 손실 함수 지정.  \n",
    "  - 분류: \n",
    "    - **\"binary:logistic\"**  \n",
    "      - 이진 분류용 (0/1).  \n",
    "      - 출력: 확률 (로지스틱 시그모이드 적용).  \n",
    "      - `predict_proba`와 연결.\n",
    "\n",
    "    - **\"binary:logitraw\"**  \n",
    "      - 이진 분류용.  \n",
    "      - 출력: 로짓(log-odds) 값 (확률 아님).  \n",
    "      - 확률 후처리 직접 하고 싶을 때 사용.\n",
    "\n",
    "    - **\"multi:softmax\"**  \n",
    "      - 다중 클래스 분류.  \n",
    "      - 출력: 클래스 인덱스 (정수).  \n",
    "\n",
    "    - **\"multi:softprob\"**  \n",
    "      - 다중 클래스 분류.  \n",
    "      - 출력: 각 클래스별 확률 벡터.  \n",
    "      - 확률 기반 후처리/평가에 적합.\n",
    "  - 회귀: \n",
    "    - **\"reg:squarederror\"** (기본)  \n",
    "      - 평균 제곱 오차(MSE) 기반.  \n",
    "      - 일반적인 연속형 타깃 예측에 사용.\n",
    "\n",
    "    - **\"reg:absoluteerror\"**  \n",
    "      - 평균 절대 오차(MAE).  \n",
    "      - 이상치(outlier)에 더 강건.\n",
    "\n",
    "    - **\"reg:squaredlogerror\"**  \n",
    "      - 로그 스케일에서의 MSE.  \n",
    "      - 타깃 값이 양수이고 값의 범위가 넓을 때 사용 (예: 가격, 소득).\n",
    "\n",
    "    - **\"reg:pseudohubererror\"**  \n",
    "      - pseudo-Huber 손실.  \n",
    "      - MSE와 MAE의 절충 → 이상치에 완화된 영향.\n",
    "\n",
    "- **eval_metric**  \n",
    "  - 검증 시 사용할 평가 지표.  \n",
    "  - 분류: `\"logloss\"`, `\"auc\"`, `\"error\"`  \n",
    "  - 회귀: `\"rmse\"`, `\"mae\"` 등  \n",
    "  - 조기 종료(early stopping)와 함께 사용.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 2. 트리 구조 제어\n",
    "- **max_depth (기본=6)**  \n",
    "  - 트리 최대 깊이. 깊으면 복잡도↑, 과적합 위험↑.  \n",
    "  - 보통 3~10 범위 탐색.\n",
    "\n",
    "- **min_child_weight (기본=1)**  \n",
    "  - 리프 노드가 분할되기 위한 최소 가중치 합(Hessian).  \n",
    "  - 값↑ → 보수적 분할(과적합↓), 값↓ → 더 세밀한 분할 허용.\n",
    "\n",
    "- **gamma (기본=0)**  \n",
    "  - 리프 추가 분할에 필요한 최소 손실 감소.  \n",
    "  - 값↑ → 불필요한 분할 억제 → 과적합 방지.\n",
    "\n",
    "- **max_leaves**  \n",
    "  - 리프 수 제한. `grow_policy=\"lossguide\"`와 함께 사용.  \n",
    "  - 깊이 대신 리프 개수로 복잡도 제어 가능.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 3. 샘플링/다양성 제어\n",
    "- **subsample (기본=1.0)**  \n",
    "  - 각 트리 학습에 사용할 샘플(행) 비율.  \n",
    "  - 일반적으로 0.6~0.9 → 트리 다양성↑, 과적합↓.\n",
    "\n",
    "- **colsample_bytree (기본=1.0)**  \n",
    "  - 각 트리에서 사용할 특징(열) 비율.  \n",
    "  - 0.5~0.9로 조정해 일반화 향상.\n",
    "\n",
    "- **colsample_bylevel / colsample_bynode (기본=1.0)**  \n",
    "  - 레벨/노드 단위에서 추가 특징 샘플링.  \n",
    "  - 미세한 규제 효과.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 4. 정규화/규제\n",
    "- **reg_lambda (lambda, 기본=1.0)**  \n",
    "  - L2 정규화(가중치 크기 억제). 값↑ → 안정적 모델, 과적합 억제.\n",
    "\n",
    "- **reg_alpha (alpha, 기본=0.0)**  \n",
    "  - L1 정규화(가중치 희소화). 값↑ → 특징 선택 효과.  \n",
    "  - 고차원 희소 데이터(예: 텍스트 분류)에 유용.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 5. 불균형 처리\n",
    "- **scale_pos_weight (기본=1)**  \n",
    "  - 양성/음성 클래스 불균형 보정.  \n",
    "  - 대략 `neg/pos` 비율로 설정. (예: 음성 90%, 양성 10% → 9)\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 6. 실행 최적화\n",
    "- **tree_method**  \n",
    "  - `\"auto\"`: 자동 선택  \n",
    "  - `\"hist\"`: 대규모 데이터 빠른 학습  \n",
    "  - `\"gpu_hist\"`: GPU 가속 → 대규모/고차원 데이터에서 권장\n",
    "\n",
    "- **max_bin (기본=256)**  \n",
    "  - 히스토그램 버킷 수. 값↑ → 정확도↑, 메모리·시간↑.\n",
    "\n",
    "- **n_jobs (기본=None)**  \n",
    "  - 병렬 처리 스레드 수. `-1` → 모든 코어 사용.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚦 하이퍼파라미터 튜닝 우선순위 가이드\n",
    "\n",
    "### ✅ 1순위 (모델 성능에 가장 큰 영향)\n",
    "- **learning_rate** ↔ **n_estimators**  \n",
    "- **max_depth**  \n",
    "- **min_child_weight**\n",
    "\n",
    "→ 트리 복잡도와 학습률/트리 개수의 조합이 성능에 직접적으로 영향.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚖️ 2순위 (과적합 방지 및 일반화 성능 개선)\n",
    "- **gamma**  \n",
    "- **subsample**  \n",
    "- **colsample_bytree**\n",
    "\n",
    "→ 분할 억제 및 샘플링 다양성을 통해 오버핏 줄이고 안정적 일반화 유도.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 3순위 (세밀한 조정/특수 상황)\n",
    "- **reg_lambda**, **reg_alpha** → 정규화 강화  \n",
    "- **scale_pos_weight** → 불균형 데이터 개선  \n",
    "- **tree_method**, **max_bin** → 속도·메모리 최적화  \n",
    "\n",
    "---\n",
    "\n",
    "## 📌 요약\n",
    "- **먼저 조정**: `learning_rate`, `n_estimators`, `max_depth`, `min_child_weight`  \n",
    "- **그 다음**: `gamma`, `subsample`, `colsample_bytree`  \n",
    "- **마지막 미세 조정**: `reg_lambda`, `reg_alpha`, `scale_pos_weight`  \n",
    "\n",
    "이 순서대로 탐색하면, 효율적으로 성능을 개선할 수 있습니다.\n",
    "\n",
    "## 3) 주요 속성 (Attributes)\n",
    "\n",
    "- **feature_importances_**: 학습된 모델의 특징 중요도\n",
    "- **best_score**, **best_iteration**, **best_ntree_limit**: 조기 종료(early stopping) 시 최적 결과\n",
    "- **evals_result()**: 학습 중 평가 지표 기록\n",
    "- **classes_** *(분류)*: 학습된 클래스 라벨 목록\n",
    "- **n_features_in_**: 입력 특징 수\n",
    "\n",
    "---\n",
    "\n",
    "## 4) 주요 메서드 (Methods)\n",
    "\n",
    "- **fit(X, y[, eval_set, early_stopping_rounds])**  \n",
    "  모델 학습. `eval_set`과 `early_stopping_rounds`로 조기 종료 가능.\n",
    "  - **eval_set** : list of (X, y), optional  \n",
    "    - 검증 데이터셋 지정  \n",
    "    - 예: `[(X_train, y_train), (X_valid, y_valid)]`  \n",
    "    - 학습 로그와 조기 종료에 사용  \n",
    "  - **verbose** : bool or int, default=None  \n",
    "    - 학습 중 로그 출력 여부/빈도  \n",
    "    - `verbose=True` → 각 라운드 성능 출력  \n",
    "    - `verbose=10` → 10 라운드마다 출력  \n",
    "- **predict(X)**  \n",
    "  분류: 클래스 라벨, 회귀: 예측 값 반환.\n",
    "\n",
    "- **predict_proba(X)** *(분류)*  \n",
    "  클래스별 확률 예측.\n",
    "\n",
    "- **score(X, y)**  \n",
    "  기본은 정확도(분류) / R²(회귀).\n",
    "\n",
    "- **get_booster()**  \n",
    "  내부 Booster 객체 반환 (저수준 API 접근).\n",
    "\n",
    "- **get_params() / set_params()**  \n",
    "  하이퍼파라미터 조회/설정.\n",
    "\n",
    "- **save_model(fname) / load_model(fname)**  \n",
    "  모델 저장/불러오기 (JSON, binary).\n",
    "\n",
    "- **evals_result()**  \n",
    "  학습 중의 평가 지표 결과를 딕셔너리로 반환.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50453bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    r2_score, mean_absolute_error, mean_squared_error\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b892949d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height_cm</th>\n",
       "      <th>weight_kg</th>\n",
       "      <th>body fat_%</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "      <th>gripForce</th>\n",
       "      <th>sit and bend forward_cm</th>\n",
       "      <th>sit-ups counts</th>\n",
       "      <th>broad jump_cm</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.0</td>\n",
       "      <td>M</td>\n",
       "      <td>172.3</td>\n",
       "      <td>75.24</td>\n",
       "      <td>21.3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>54.9</td>\n",
       "      <td>18.4</td>\n",
       "      <td>60.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age gender  height_cm  weight_kg  body fat_%  diastolic  systolic  \\\n",
       "0  27.0      M      172.3      75.24        21.3       80.0     130.0   \n",
       "\n",
       "   gripForce  sit and bend forward_cm  sit-ups counts  broad jump_cm class  \n",
       "0       54.9                     18.4            60.0          217.0     C  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 파일 경로를 맞게 변경하세요.\n",
    "body = pd.read_csv(\"./data/bodyPerformance.csv\")\n",
    "body.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3a9363e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.35549\n",
      "[50]\tvalidation_0-mlogloss:0.83926\n",
      "[100]\tvalidation_0-mlogloss:0.72915\n",
      "[150]\tvalidation_0-mlogloss:0.68269\n",
      "[200]\tvalidation_0-mlogloss:0.65806\n",
      "[250]\tvalidation_0-mlogloss:0.64306\n",
      "[300]\tvalidation_0-mlogloss:0.63351\n",
      "[350]\tvalidation_0-mlogloss:0.62767\n",
      "[400]\tvalidation_0-mlogloss:0.62371\n",
      "[450]\tvalidation_0-mlogloss:0.62163\n",
      "[500]\tvalidation_0-mlogloss:0.61934\n",
      "[550]\tvalidation_0-mlogloss:0.61833\n",
      "[600]\tvalidation_0-mlogloss:0.61769\n",
      "[650]\tvalidation_0-mlogloss:0.61753\n",
      "[680]\tvalidation_0-mlogloss:0.61812\n",
      "=== bodyPerformance: 다중 분류 ===\n",
      "Best iteration: 630\n",
      "Accuracy: 0.7521463232549459\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7193    0.8836    0.7930       670\n",
      "           1     0.6331    0.6009    0.6166       669\n",
      "           2     0.7444    0.6910    0.7167       670\n",
      "           3     0.9316    0.8328    0.8794       670\n",
      "\n",
      "    accuracy                         0.7521      2679\n",
      "   macro avg     0.7571    0.7521    0.7514      2679\n",
      "weighted avg     0.7571    0.7521    0.7515      2679\n",
      "\n",
      "Confusion matrix:\n",
      " [[592  70   7   1]\n",
      " [165 402  84  18]\n",
      " [ 57 128 463  22]\n",
      " [  9  35  68 558]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 타깃 인코딩 (A,B,C,D → 0,1,2,3)\n",
    "class_map = {\"A\":0, \"B\":1, \"C\":2, \"D\":3}\n",
    "body[\"target\"] = body[\"class\"].map(class_map)\n",
    "body['gender'] = np.where(body['gender'] == 'M', 0, 1)\n",
    "# 입력/타깃 분리\n",
    "X_body = body.drop(columns=[\"class\", \"target\"])\n",
    "y_body = body[\"target\"]\n",
    "\n",
    "\n",
    "Xb_train, Xb_valid, yb_train, yb_valid = train_test_split(\n",
    "    X_body, y_body, test_size=0.2, random_state=42, stratify=y_body\n",
    ")\n",
    "\n",
    "# XGBoost 분류기 (다중분류) — 조기 종료 사용\n",
    "clf = XGBClassifier(\n",
    "    objective=\"multi:softprob\",   # 확률 출력\n",
    "    eval_metric=\"mlogloss\",\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    min_child_weight=2,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    early_stopping_rounds=50,   \n",
    "    reg_lambda=2.0,\n",
    "    random_state=42,\n",
    "    tree_method=\"hist\"            # 대체: \"gpu_hist\" (GPU 사용 시)\n",
    ")\n",
    "\n",
    "clf.fit(\n",
    "    Xb_train, yb_train, \n",
    "    eval_set=[(Xb_valid, yb_valid)], verbose=50\n",
    ")\n",
    "\n",
    "yb_pred = clf.predict(Xb_valid)\n",
    "print(\"=== bodyPerformance: 다중 분류 ===\")\n",
    "print(\"Best iteration:\", clf.best_iteration)\n",
    "print(\"Accuracy:\", accuracy_score(yb_valid, yb_pred))\n",
    "print(\"\\nClassification report:\\n\", classification_report(yb_valid, yb_pred, digits=4))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(yb_valid, yb_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "756263be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:7656.65072\n",
      "[100]\tvalidation_0-rmse:2394.08758\n",
      "[200]\tvalidation_0-rmse:2240.15496\n",
      "[300]\tvalidation_0-rmse:2197.80429\n",
      "[400]\tvalidation_0-rmse:2185.63579\n",
      "[481]\tvalidation_0-rmse:2191.59692\n",
      "\n",
      "=== CarPrice_Assignment: 회귀 ===\n",
      "Best iteration: 382\n",
      "RMSE: 4765214.3643 | MAE: 1382.2057 | R2: 0.9211\n"
     ]
    }
   ],
   "source": [
    "car = pd.read_csv(\"./data/CarPrice_Assignment.csv\")\n",
    "\n",
    "car_num = car.select_dtypes(['number'])\n",
    "features = list(car_num.columns.difference(['car_ID', 'symboling', 'price']))\n",
    "\n",
    "x = car_num[features]\n",
    "y = car_num['price']\n",
    "\n",
    "## 데이터 분할 \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xc_train, Xc_valid, yc_train, yc_valid = train_test_split(x, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# XGBoost 회귀기 — 조기 종료 사용\n",
    "reg = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    eval_metric=\"rmse\",\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=6,\n",
    "    min_child_weight=3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=3.0,\n",
    "    random_state=42,\n",
    "    tree_method=\"hist\",\n",
    "    early_stopping_rounds=100\n",
    ")\n",
    "\n",
    "reg.fit(\n",
    "    Xc_train, yc_train, eval_set=[(Xc_valid, yc_valid)], verbose=100\n",
    ")\n",
    "\n",
    "yc_pred = reg.predict(Xc_valid)\n",
    "rmse = mean_squared_error(yc_valid, yc_pred)\n",
    "mae = mean_absolute_error(yc_valid, yc_pred)\n",
    "r2  = r2_score(yc_valid, yc_pred)\n",
    "\n",
    "print(\"\\n=== CarPrice_Assignment: 회귀 ===\")\n",
    "print(\"Best iteration:\", reg.best_iteration)\n",
    "print(f\"RMSE: {rmse:.4f} | MAE: {mae:.4f} | R2: {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
